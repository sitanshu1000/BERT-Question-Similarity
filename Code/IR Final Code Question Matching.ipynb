{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljJCeJ2hRCYM",
    "outputId": "779498fb-993d-47a7-c419-0d5e2ac6a7b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 28.0MB/s \n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 38.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 41.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Installing collected packages: tokenizers, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mmIPaMQp7lOi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGHsxDbr2Fak",
    "outputId": "2802c03b-65d4-419e-a69a-d3fda9bcad7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "fwnMgnyqkWe1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import multiprocessing\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIfIwHeeyUNa",
    "outputId": "797213a7-f5a4-4323-c230-2619a04a23c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU(s) available:  1  GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# GPU availability\n",
    "if torch.cuda.is_available():    \n",
    "    # use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU(s) available: ',torch.cuda.device_count(), ' GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIB5DeigznXk"
   },
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eAfW9-hvnYNm",
    "outputId": "3778e1ba-cbc2-4d1e-a6cd-96c35416d089"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 282963 entries, 0 to 282962\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   Unnamed: 0    282963 non-null  int64 \n",
      " 1   question1     282963 non-null  object\n",
      " 2   question2     282962 non-null  object\n",
      " 3   is_duplicate  282963 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 8.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "qa_df = pd.read_csv('//Extra/train.csv')\n",
    "qa_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "-VZ-K9RrAxx1"
   },
   "outputs": [],
   "source": [
    "\n",
    "def text_clean(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    stemmer= PorterStemmer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCB3ve_tAya_",
    "outputId": "750d80e3-c92a-49fe-a2d2-2baf0616cf8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "qa_df['question1']=qa_df['question1'].apply(lambda x:text_clean(x))\n",
    "qa_df['question2']=qa_df['question2'].apply(lambda x:text_clean(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "lU4CPDXMj2OG",
    "outputId": "a945b857-c34b-4ea3-929e-00e0a8901a15"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>who is the most over rated actress in hindi ci...</td>\n",
       "      <td>who is the most overrated actress in bollywood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>what should every traveler know in order not t...</td>\n",
       "      <td>what should every traveller know in order to n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>how can i name these organic compounds</td>\n",
       "      <td>what are the names of these organic compounds</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>how many hostels are there in the dayanand sag...</td>\n",
       "      <td>what is the minimum hostel fees in ssn college...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>should i read naruto manga or watch the anime</td>\n",
       "      <td>where can you read naruto manga</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ... is_duplicate\n",
       "0           0  ...            1\n",
       "1           1  ...            0\n",
       "2           2  ...            0\n",
       "3           3  ...            0\n",
       "4           4  ...            0\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dMJwX_qzn7uG",
    "outputId": "61a1eb73-99d5-469e-9131-a3b1e8e8b860"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, question1, question2, is_duplicate]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "nan_values = qa_df[qa_df.isna().any(axis=1)]\n",
    "print(nan_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejZpIeu-qPoZ",
    "outputId": "23bc1d5f-7981-4215-b6af-19a3c9108e3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.631348\n",
       "1    0.368652\n",
       "Name: is_duplicate, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove NAN rows\n",
    "qa_df.dropna(inplace=True)\n",
    "qa_df['is_duplicate'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwPpdx_xqvIk",
    "outputId": "ccf03624-aa11-4402-ab50-26e85896277f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.6298\n",
       "1    0.3702\n",
       "Name: is_duplicate, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random sample of 5000 rows\n",
    "qa_sample = qa_df[['question1','question2','is_duplicate']].sample(n=5000, random_state=1)\n",
    "# checking duplicate question distribution\n",
    "qa_sample['is_duplicate'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orhdfwjJzu6w"
   },
   "source": [
    "Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "fZ1Ic5MdxP5O"
   },
   "outputs": [],
   "source": [
    "# stratified splitting for approx similar proportion of duplicate questions through the training and test sets\n",
    "# train & test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(qa_sample[['question1','question2']], qa_sample['is_duplicate'], test_size=0.2, random_state=4, stratify=qa_sample['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "mduzr8nfHIUg"
   },
   "outputs": [],
   "source": [
    "# train & validation dataset\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train[['question1','question2']], y_train, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aW_nU9pxs1nq"
   },
   "source": [
    "Tokenizing & Creating Dataloaders for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "zCBIftftQmEM"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained bert model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMR-N-KlRfNj",
    "outputId": "9051ea3b-0b34-4b5d-f4dd-4d4348c78aea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 10553.00it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 10730.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Maximum Length:  113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# find maximum length to apply padding\n",
    "tqdm.pandas()\n",
    "q1_len = qa_sample[\"question1\"].progress_apply(lambda q1: len(tokenizer.tokenize(q1))) # question1 lengths\n",
    "q2_len = qa_sample[\"question2\"].progress_apply(lambda q2: len(tokenizer.tokenize(q2))) # question2 lengths\n",
    "total_len = q1_len + q2_len\n",
    "max_len = total_len.max()\n",
    "print(\"\\n\\n Maximum Length: \",max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "IKuIgzuIYcd1"
   },
   "outputs": [],
   "source": [
    "# tokenize & convert to tensors\n",
    "def convert_to_dataset_torch(data: pd.DataFrame, labels: pd.Series) -> TensorDataset:\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "  token_type_ids = []\n",
    "  for _, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    # create tokens, add CLS & SEP and encode\n",
    "    encoded_dict = tokenizer.encode_plus(row[\"question1\"], row[\"question2\"], max_length=max_len, \n",
    "                                         padding='max_length', return_attention_mask=True, \n",
    "                                         return_tensors='pt', truncation=True)\n",
    "    # token ids\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    # \n",
    "    token_type_ids.append(encoded_dict[\"token_type_ids\"])\n",
    "    # attention mask - differentiates padding from non-padding\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "  # Convert to  tensors\n",
    "  input_ids = torch.cat(input_ids, dim=0)\n",
    "  token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "  attention_masks = torch.cat(attention_masks, dim=0)\n",
    "  labels = torch.tensor(labels.values)\n",
    "  return TensorDataset(input_ids, attention_masks, token_type_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBfturxQY9Tm",
    "outputId": "437dc5f8-ebb6-4266-bdba-ef51379de519"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3200/3200 [00:01<00:00, 2791.45it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 2738.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# converting training dataset\n",
    "train = convert_to_dataset_torch(x_train, y_train)\n",
    "# converting validation dataset\n",
    "validation = convert_to_dataset_torch(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Z1F2KaU_ZZJV"
   },
   "outputs": [],
   "source": [
    "# batch size for Dataloader\n",
    "batch_size = 32\n",
    "core_number = multiprocessing.cpu_count()\n",
    "\n",
    "# DataLoaders for training set \n",
    "train_dataloader = DataLoader(train,  # training samples\n",
    "                              sampler = RandomSampler(train), # random batches\n",
    "                              batch_size = batch_size, # batch size\n",
    "                              num_workers = core_number\n",
    "                              )\n",
    "\n",
    "# DataLoaders for validation set\n",
    "validation_dataloader = DataLoader(validation, # validation samples\n",
    "                                   sampler = SequentialSampler(validation), # sequential batches \n",
    "                                   batch_size = batch_size, # batch size\n",
    "                                   num_workers = core_number\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLyiBdc22oPN"
   },
   "source": [
    "Loading Pre-trained Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVby_4YLZjj7",
    "outputId": "c0c3284d-93f1-42a5-d3dc-83a076f9307e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification the pretrained BERT model \n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # 12-layer BERT model with an uncased vocab\n",
    "    num_labels=2, # number of output labels   \n",
    "    output_attentions=False, # Whether the model returns attentions weights\n",
    "    output_hidden_states=False, # Whether the model returns all hidden-states\n",
    ")\n",
    "\n",
    "# run this model on the GPU\n",
    "bert_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "1wfsNnxz39lS"
   },
   "outputs": [],
   "source": [
    "# Optimizer AdamW (class from the huggingface library)\n",
    "adamw_optimizer = AdamW(bert_model.parameters(),\n",
    "                  lr = 2e-5, # learning_rate \n",
    "                  eps = 1e-8 # adam_epsilon  \n",
    "                )\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 2\n",
    "\n",
    "# Total number of training steps \n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(adamw_optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps1b02amXMIL"
   },
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "lvWm6-OGXLz4"
   },
   "outputs": [],
   "source": [
    "# fitting model on training data using dataloader\n",
    "def fit_batch(dataloader, model, optimizer, epoch):\n",
    "  total_train_loss = 0\n",
    "  \n",
    "  for batch in tqdm(dataloader, desc=f\"Training epoch:{epoch}\", unit=\"batch\"):\n",
    "    # Unpack batch from dataloader\n",
    "    input_ids = batch[0].to(device)\n",
    "    attention_masks = batch[1].to(device)\n",
    "    token_type_ids = batch[2].to(device)\n",
    "    labels = batch[3].to(device)       \n",
    "    # clear previously calculated gradients before backward pass\n",
    "    model.zero_grad()\n",
    "    # forward pass \n",
    "    op = model(input_ids, \n",
    "                    token_type_ids=token_type_ids, \n",
    "                    attention_mask=attention_masks, \n",
    "                    labels=labels)\n",
    "    loss = op[0]\n",
    "    total_train_loss += loss.item()\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "    # Clip the norm of the gradients to prevent the \"exploding gradients\" problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    # Update parameters \n",
    "    optimizer.step()\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "        \n",
    "  return total_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "4f_3fhwuXXUF"
   },
   "outputs": [],
   "source": [
    "# evaluating with validation dataset\n",
    "def eval_batch(dataloader, model, metric=accuracy_score):\n",
    "  total_eval_accuracy = 0\n",
    "  total_eval_loss = 0\n",
    "  predictions , predicted_labels = [], []\n",
    "    \n",
    "  for batch in tqdm(dataloader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "    # Unpack batch from dataloader\n",
    "    input_ids = batch[0].to(device)\n",
    "    attention_masks = batch[1].to(device)\n",
    "    token_type_ids = batch[2].to(device)\n",
    "    labels = batch[3].to(device)\n",
    "        \n",
    "    # not to bother with constructing the compute graph during the forward pass\n",
    "    with torch.no_grad():\n",
    "      # Forward pass\n",
    "      op = model(input_ids, \n",
    "                 token_type_ids=token_type_ids, \n",
    "                 attention_mask=attention_masks,\n",
    "                 labels=labels)\n",
    "      loss, logits = op[:2]\n",
    "    \n",
    "    total_eval_loss += loss.item()\n",
    "    # accuracy for this batch and accumulate it over all batches\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = labels.to('cpu').numpy()\n",
    "    y_pred = np.argmax(logits, axis=1).flatten()\n",
    "    total_eval_accuracy += metric(label_ids, y_pred)\n",
    "    predictions.extend(logits.tolist())\n",
    "    predicted_labels.extend(y_pred.tolist())\n",
    "    \n",
    "  return total_eval_accuracy, total_eval_loss, predictions, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "3253O42Wbms1"
   },
   "outputs": [],
   "source": [
    "# Set the seed value to make this reproducible.\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "\n",
    "# training \n",
    "def train(train_dataloader, validation_dataloader, model, optimizer, epochs):\n",
    "    training_stats = []\n",
    "    \n",
    "    for epoch in range(0, epochs):        \n",
    "        # Reset the total loss for epoch\n",
    "        total_train_loss = 0\n",
    "        # model into training mode \n",
    "        model.train()\n",
    "        # fitting on training dataset\n",
    "        total_train_loss = fit_batch(train_dataloader, model, optimizer, epoch)\n",
    "        # Calculate the average loss over all of the batches\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        # model in evaluation mode\n",
    "        model.eval()\n",
    "        # evaluate on validation dataset\n",
    "        total_eval_accuracy, total_eval_loss, _, _ = eval_batch(validation_dataloader, model)\n",
    "        # final accuracy for this validation run\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(f\"  Accuracy: {avg_val_accuracy}\")\n",
    "        # average loss over all of the batches\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        print(f\"  Validation Loss: {avg_val_loss}\")\n",
    "        \n",
    "        # statistics from this epoch\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    print(\"\\nTraining complete!\")\n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3yEpySm77RSp",
    "outputId": "530c68d8-11bd-422d-a023-b6647662ae86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7iTZCXfY66x",
    "outputId": "13b4104d-67fb-4062-b22d-3957e1ccdb30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch:0: 100%|██████████| 100/100 [01:02<00:00,  1.60batch/s]\n",
      "Evaluating: 100%|██████████| 25/25 [00:05<00:00,  4.36batch/s]\n",
      "Training epoch:1:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.69125\n",
      "  Validation Loss: 0.5093021655082702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch:1: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s]\n",
      "Evaluating: 100%|██████████| 25/25 [00:05<00:00,  4.27batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.7425\n",
      "  Validation Loss: 0.47197970747947693\n",
      "\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "training_stats = train(train_dataloader, validation_dataloader, bert_model, adamw_optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "ijl9pMBwnwwo",
    "outputId": "7c8f9c0c-9625-4573-a9cd-ec525ab42516"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.553143</td>\n",
       "      <td>0.509302</td>\n",
       "      <td>0.69125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.414490</td>\n",
       "      <td>0.471980</td>\n",
       "      <td>0.74250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur.\n",
       "epoch                                           \n",
       "0           0.553143     0.509302        0.69125\n",
       "1           0.414490     0.471980        0.74250"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training statistics for each epoch\n",
    "train_stats = pd.DataFrame(training_stats).set_index('epoch')\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "UMBw8z0hddcG",
    "outputId": "c1eb40f3-ba22-4301-93bc-57afd8e47b1e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing training & validation loss\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(train_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(train_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks(train_stats.index.values.tolist())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRtmQMUd38JS"
   },
   "source": [
    "Performance on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t060Z2EgdpiQ",
    "outputId": "8009b44c-bc75-4ead-ce84-47ad837609c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2688.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# test dataset\n",
    "test = convert_to_dataset_torch(x_test, y_test)\n",
    "# test set dataloader\n",
    "test_dataloader = DataLoader(test, sampler=SequentialSampler(test), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMPG0v9adt7v",
    "outputId": "53c30868-989b-4556-b2d6-91128dfc37ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32/32 [00:07<00:00,  4.53batch/s]\n"
     ]
    }
   ],
   "source": [
    "bert_model.eval()\n",
    "# predict labels for test set\n",
    "_, _,_ ,predicted_labels = eval_batch(test_dataloader, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3V1O6vzkdyMz",
    "outputId": "6b77169e-a472-47a0-fd79-b8aa1724df9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    0    1\n",
      "Actual             \n",
      "0          412  218\n",
      "1           41  329\n"
     ]
    }
   ],
   "source": [
    "# testing performance\n",
    "dt = {'y_Actual': y_test,\n",
    "      'y_Predicted': predicted_labels\n",
    "      }\n",
    "df = pd.DataFrame(dt, columns=['y_Actual','y_Predicted'])\n",
    "confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YmUfZ1Nqd136",
    "outputId": "fe2ab1ee-5e25-41e3-99ea-c08dae01b432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content/model_save/tokenizer_config.json',\n",
       " '/content/model_save/special_tokens_map.json',\n",
       " '/content/model_save/vocab.txt',\n",
       " '/content/model_save/added_tokens.json')"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"__file__\").parents[0].absolute().joinpath(\"model_save\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "# Save a trained model, configuration and tokenizer \n",
    "model_to_save = bert_model.module if hasattr(bert_model, 'module') else bert_model  \n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(str(output_dir.absolute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "G89yuYXS-9Rx"
   },
   "outputs": [],
   "source": [
    "def QuestionQuestionChecking():\n",
    "    q1 = text_clean(input(\"Enter the Question 1 : \"))\n",
    "    q2 = text_clean(input(\"Enter the Question 2 : \"))\n",
    "    encoded_dict = tokenizer.encode_plus(q1, q2, max_length=max_len, \n",
    "                                         padding='max_length', return_attention_mask=True, \n",
    "                                         return_tensors='pt', truncation=True)\n",
    "    bert_model.eval()\n",
    "    input_ids = encoded_dict['input_ids'].to(device)\n",
    "    attention_masks = encoded_dict['attention_mask'].to(device)\n",
    "    token_type_ids = encoded_dict['token_type_ids'].to(device)\n",
    "    op = bert_model(input_ids, \n",
    "                 token_type_ids=token_type_ids, \n",
    "                 attention_mask=attention_masks)\n",
    "    logits = op[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    y_pred = np.argmax(logits)\n",
    "    if y_pred == 1:\n",
    "        print(\"The questions are duplicate\")\n",
    "    else:\n",
    "        print(\"The questions are different\")\n",
    "    decision = input(\"Do you want to continue [Y/N]?\")\n",
    "    if decision.strip() == 'N':\n",
    "        print(\"Ok bye\")\n",
    "    else:\n",
    "        QuestionQuestionChecking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Od1PKhWa_D7R",
    "outputId": "e84d87bb-8be6-4272-d0e2-5dcbaf8174d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Question 1 : How to learn python ?\n",
      "Enter the Question 2 : what is the best way to learn python ?\n",
      "The questions are duplicate\n",
      "Do you want to continue [Y/N]?Y\n",
      "Enter the Question 1 : What is a bitcoin ? \n",
      "Enter the Question 2 : explain Radiation ?\n",
      "The questions are different\n",
      "Do you want to continue [Y/N]?N\n",
      "Ok bye\n"
     ]
    }
   ],
   "source": [
    "QuestionQuestionChecking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-5zrILgAlcr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IR_Project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
